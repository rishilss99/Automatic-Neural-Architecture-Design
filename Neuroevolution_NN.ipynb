{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing NN architecture using Neuroevolution\n",
    "Custom pipeline for model instantiation and neuroevolution procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Must Remember points  \n",
    "**__ getattribute __ can be used to convert string to a method or class call**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn import model_selection\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_iris()\n",
    "x = data['data']\n",
    "y = data['target']\n",
    "\n",
    "x_train, x_val, y_train, y_val = model_selection.train_test_split(data.data,data.target,test_size = 0.2)\n",
    "\n",
    "x_tr_ten = torch.FloatTensor(x_train)\n",
    "y_tr_ten = torch.LongTensor(y_train)\n",
    "\n",
    "trainloader = TensorDataset(x_tr_ten,y_tr_ten)\n",
    "\n",
    "x_te_ten = torch.FloatTensor(x_val)\n",
    "y_te_ten = torch.LongTensor(y_val)\n",
    "\n",
    "testloader = TensorDataset(x_te_ten,y_te_ten)\n",
    "\n",
    "trainer = DataLoader(trainloader,batch_size=5)\n",
    "tester = DataLoader(testloader,batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Experiments\n",
    "\n",
    "# a = \"sigmoid\"\n",
    "# b = torch.__getattribute__(a)\n",
    "\n",
    "# t = b(torch.Tensor([1,0,9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self,layer_1,layer_2,activation_1,activation_2):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden1 = nn.Linear(4,layer_1)\n",
    "        \n",
    "        self.hidden2 = nn.Linear(layer_1,layer_2)\n",
    "        \n",
    "        self.output = nn.Linear(layer_2,3)\n",
    "        \n",
    "        self.activation1 = torch.__getattribute__(activation_1)\n",
    "        \n",
    "        self.activation2 = torch.__getattribute__(activation_2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = x.view(-1,4)\n",
    "        \n",
    "        x = self.activation1(self.hidden1(x))\n",
    "        \n",
    "        x = self.activation2(self.hidden2(x))\n",
    "        \n",
    "        x = torch.log_softmax(self.output(x),dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_params(input_list):\n",
    "    \n",
    "    activation_fn = [\"sigmoid\",\"relu\",\"tanh\"]\n",
    "    optimizer_fn = [\"Adam\",\"SGD\",\"LBFGS\"]\n",
    "    \n",
    "    layer1 = input_list[0]\n",
    "    \n",
    "    layer2 = input_list[1]\n",
    "    \n",
    "    activation1 = activation_fn[math.floor(input_list[2])]\n",
    "    \n",
    "    activation2 = activation_fn[math.floor(input_list[3])]\n",
    "    \n",
    "    learning_rate = input_list[4]\n",
    "    \n",
    "    optimizer = optimizer_fn[math.floor(input_list[5])]\n",
    "    \n",
    "    return layer1, layer2, activation1, activation2, learning_rate, optimizer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(params_list):\n",
    "    \n",
    "    \"\"\"The function takes the input of the floating number encoded chromosome and returns the accuracy as thr output\"\"\"\n",
    "    \n",
    "    layer1, layer2, activation1, activation2, learning_rate, optimizer = convert_params(params_list)\n",
    "    \n",
    "    classifier = NN_Model(layer1, layer2, activation1, activation2)\n",
    "    \n",
    "    if torch.cuda.is_available:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    classifier.to(device)\n",
    "\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    opt = torch.optim.__getattribute__(optimizer)\n",
    "    \n",
    "    optimizer = opt(classifier.parameters(),lr=learning_rate)\n",
    "    \n",
    "    train(classifier,optimizer,loss_function,trainer,tester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.7777777777778\n"
     ]
    }
   ],
   "source": [
    "get_accuracy([6,8,0.5,1.7,0.03,1.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer,loss_fun,trainloader,testloader,epochs=15,device=\"cuda\"):\n",
    "    \n",
    "#     tr_list = []\n",
    "#     val_list = []\n",
    "    acc_list = []\n",
    "#     epoch_list = [i+1 for i in range(epochs)]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "#         training_loss = 0\n",
    "#         validation_loss = 0\n",
    "        model.train()                #--------------------->Allows for parameters to be updated by backpropagation\n",
    "        \n",
    "        for batch in trainloader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            inputs,labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fun(outputs,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "#             training_loss += loss.item()\n",
    "        \n",
    "        model.eval()              #------------------------>Freezes the parameters for model validation\n",
    "        correct_pred = 0\n",
    "        total_pred = 0\n",
    "        \n",
    "        for batch in testloader:\n",
    "            \n",
    "            inputs,labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fun(outputs,labels)\n",
    "            \n",
    "#             validation_loss +=loss.item()\n",
    "            \n",
    "            ps = torch.exp(outputs)         #-------------->The final activation function is Log_Softmax that's why we take exp\n",
    "            \n",
    "            correct = torch.eq(torch.max(ps,dim=1)[1],labels).view(-1)\n",
    "            \n",
    "            correct_pred += torch.sum(correct).item()\n",
    "            total_pred += correct.shape[0]       #--------->Alternatively can also write batch.shape[0]\n",
    "            \n",
    "#         training_loss = training_loss/len(trainloader)\n",
    "#         validation_loss = validation_loss/len(testloader)\n",
    "        \n",
    "#         tr_list.append(training_loss)\n",
    "#         val_list.append(validation_loss)\n",
    "        acc_list.append((correct_pred*100.0/total_pred))\n",
    "    \n",
    "    print(sum(acc_list)/len(acc_list))\n",
    "            \n",
    "#         print(\"Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}\"\n",
    "#               .format(epoch+1, training_loss,validation_loss, correct_pred * 100.0 / total_pred))\n",
    "    \n",
    "#     fig = plt.figure(figsize=(20,6))\n",
    "        \n",
    "#     plt.subplot(1,3,1)\n",
    "#     plt.plot(epoch_list,tr_list)\n",
    "#     plt.title(\"Training Loss\")\n",
    "    \n",
    "#     plt.subplot(1,3,2)\n",
    "#     plt.plot(epoch_list,val_list)\n",
    "#     plt.title(\"Validation Loss\")\n",
    "    \n",
    "#     plt.subplot(1,3,3)\n",
    "#     plt.plot(epoch_list,acc_list)\n",
    "#     plt.title(\"Accuracy\")\n",
    "    \n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
